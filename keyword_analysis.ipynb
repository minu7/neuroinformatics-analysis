{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "metadata": {
    "interpreter": {
     "hash": "1d6699aa86f2602ea533c86e4d7a2e97a146b603848a19e03a1edb8b897b9b3d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "# Init the connection to the database\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"neuroinformatics\"), encrypted=False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def cyperQueryToDataFrame(query):\n",
    "  with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    return pd.DataFrame(result.data(), columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = cyperQueryToDataFrame(\"MATCH (n:Keyword) RETURN n.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-1.0.2.tar.gz (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
      "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
      "  Downloading transformers-4.4.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 14.9 MB/s \n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 7.5 MB/s \n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.8.0-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 119.6 MB 11.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sentence-transformers) (1.6.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 11.5 MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 15.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: packaging in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 15.3 MB/s \n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.25.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.3.17-cp38-cp38-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 16.1 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 17.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: six in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/filippo/.pyenv/versions/3.8.7/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Building wheels for collected packages: sentence-transformers, nltk, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.2-py3-none-any.whl size=114269 sha256=d4bed975b776e1c681b4cb4d4aa44386afdbbeed9535708dd8fb07497ed4447c\n",
      "  Stored in directory: /Users/filippo/Library/Caches/pip/wheels/83/9e/62/b90b3419c1beea3c675a7e86af99c302b8bd33f4b6c08a2b1a\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=9cd7d85f31f2ffbc1bcc1f929374347bbc16c73cb72bba4d5adc5fed55bb51cb\n",
      "  Stored in directory: /Users/filippo/Library/Caches/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=e363a3e8e1a6f9ef4a1a3421c6c5b6f63296760ed95faecc05ae58cbd02c3a40\n",
      "  Stored in directory: /Users/filippo/Library/Caches/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sentence-transformers nltk sacremoses\n",
      "Installing collected packages: tqdm, regex, tokenizers, sacremoses, filelock, transformers, torch, sentencepiece, nltk, sentence-transformers\n",
      "Successfully installed filelock-3.0.12 nltk-3.5 regex-2021.3.17 sacremoses-0.0.43 sentence-transformers-1.0.2 sentencepiece-0.1.95 tokenizers-0.10.1 torch-1.8.0 tqdm-4.59.0 transformers-4.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: -0.0686\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0891\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9907\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('stsb-roberta-base') # roberta base good and not too large\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return ' '.join([m.group(0) for m in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = keywords['n.id'].squeeze().map(lambda x: ' '.join(camel_case_split(x[:-4]).split('_'))).tolist() # rimozione camelcase e underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          n.id\n",
       "0                                    Brain_sub\n",
       "1       ComputingMilieux_PERSONALCOMPUTING_sub\n",
       "2                   Biomedical Engineering_sub\n",
       "3                                     fMRI_sub\n",
       "4                                    dfMRI_sub\n",
       "...                                        ...\n",
       "192882      CORTICOTROPIN-RELEASING FACTOR_sub\n",
       "192883                      COLOCALIZATION_sub\n",
       "192884     Glutathione S-Transferase Alpha_sub\n",
       "192885        Glutathione S-Transferase pi_sub\n",
       "192886                high‐density montage_sub\n",
       "\n",
       "[192887 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n.id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Brain_sub</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ComputingMilieux_PERSONALCOMPUTING_sub</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Biomedical Engineering_sub</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fMRI_sub</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dfMRI_sub</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>192882</th>\n      <td>CORTICOTROPIN-RELEASING FACTOR_sub</td>\n    </tr>\n    <tr>\n      <th>192883</th>\n      <td>COLOCALIZATION_sub</td>\n    </tr>\n    <tr>\n      <th>192884</th>\n      <td>Glutathione S-Transferase Alpha_sub</td>\n    </tr>\n    <tr>\n      <th>192885</th>\n      <td>Glutathione S-Transferase pi_sub</td>\n    </tr>\n    <tr>\n      <th>192886</th>\n      <td>high‐density montage_sub</td>\n    </tr>\n  </tbody>\n</table>\n<p>192887 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_import = []\n",
    "for i in range(len(sentences)):\n",
    "    json_to_import.append({ \"id\": keywords['n.id'][i], \"embeddings\": embeddings[i].tolist(),  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json_to_import).to_csv('embeddings_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Init the connection to the database\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"neuroinformatics\"), encrypted=False)\n",
    "session = driver.session()\n",
    "for i in range(len(sentences)):\n",
    "    session.run(\"MATCH (n:Keyword) WHERE n.id = $id SET n.r_embeddings = $embeddings RETURN n\", { \"id\": keywords['n.id'][i], \"embeddings\": embeddings[i].tolist() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}